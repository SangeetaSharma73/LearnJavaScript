# 🚀 Understanding Streams in Node.js

## ✅ Why use streams?

When you read a large file with fs.readFile(), Node tries to load the entire file into memory at once.
That works for small files, but will crash your server if you try it with a huge video or log file.

### 👉 Streams solve this problem by:

- reading data in chunks
- processing data while reading it
- using very little memory

## ✅ How streams work

In Node.js, a stream is like a pipe:

- Data flows through the pipe
- You can read it chunk by chunk
- You can also pause, resume, or pipe it elsewhere (like to a network socket or another file)

## There are four types of streams:

- Readable (e.g. reading a file)
- Writable (e.g. writing to a file)
- Duplex (both read/write)
- Transform (modify data on the fly)

For your assignment, you’ll use a Readable stream to read a large file.

## ✅ Conceptual steps to solve this assignment

### 1️⃣ Create a readable stream

- use fs.createReadStream()
- give it a path to a large file
- optionally specify the highWaterMark (chunk size)

### 2️⃣ Listen for events on the stream

Streams emit events:

- data — fires for each chunk
- end — fires when the whole stream is done
- error — fires if there’s a problem

### 3️⃣ Track progress

- Each time you get a chunk (data event), measure its size

- Add the chunk size to a running total

- Print out progress in percentage

### 4️⃣ On end event

- log “reading finished”

- show the total size you read

### ✅ Summary of what you’d do

✔ Import the fs module
✔ Create a readable stream from a large file
✔ Listen for data to get each chunk
✔ Use .length to measure how much data you got so far
✔ Listen for end to know when it’s done
✔ Optionally listen for error

### ✅ Things to remember

- Streams are asynchronous
- They work chunk-by-chunk
- They emit events
- They help handle large data with minimal memory

### 🚀 What you should build

👉 A program that:

```txt
opens a large file with createReadStream()

logs a progress message for every chunk (e.g., “Read 20% done”)

finally logs a “complete” message at the end
```

## 🚀 Node.js Stream Example: Reading a large file and showing progress

Imagine you have a large text file called bigfile.txt.
We will read it in chunks, log how much we have read so far, and show progress as a percentage.

### Step-by-Step Explanation

// 1️⃣ Load the required module

const fs = require('fs');
👉 We need Node’s fs module to handle file operations.

// 2️⃣ Find the file size first so we can calculate percentage
const filePath = 'bigfile.txt';
const stats = fs.statSync(filePath);
const totalSize = stats.size;

👉fs.statSync synchronously gets file info (like its size in bytes).
totalSize will help us track progress later.

// 3️⃣ Create a readable stream from the file
const readStream = fs.createReadStream(filePath);

👉
This returns a Readable stream.

Node will automatically break the file into chunks (default chunk size is 64 KB).

These chunks will be sent through the stream, event by event.

// 4️⃣ Set up variables to track progress
let bytesRead = 0;
👉 We’ll accumulate how many bytes we have read so far.

// 5️⃣ Listen for 'data' events, which give us each chunk
readStream.on('data', (chunk) => {
bytesRead += chunk.length; // accumulate how many bytes we’ve read so far
const percent = ((bytesRead / totalSize) \* 100).toFixed(2); // calculate percentage
console.log(`Progress: ${percent}%`);
});
👉
Every time data arrives, this callback fires.

chunk is a Buffer of data

chunk.length tells us its size in bytes

we keep a running total with bytesRead

we calculate the progress as a percentage

// 6️⃣ Listen for the 'end' event
readStream.on('end', () => {
console.log('✅ File reading finished!');
});
👉
Fires after all chunks have been read

Perfect place to log “all done”.

// 7️⃣ Listen for any errors
readStream.on('error', (err) => {
console.error('❌ Error reading file:', err);
});

👉 Always handle errors with streams — e.g. if file is missing.

## ✅ In Plain English

✔ You open the file with createReadStream
✔ You listen for chunk events with data
✔ You add up the chunk sizes to track progress
✔ You log the percentage
✔ When finished, the end event fires
